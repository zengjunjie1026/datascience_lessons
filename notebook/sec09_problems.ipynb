{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 09: K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `wine` dateset from `sklearn`. Scale with `StandardScalar` and store the scaled data. Make a scatterplot of `color_intensity` against `alcohol` with points colored by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run K-Means Clustering using different numbers of clusters (1â€”20) and make an elbow plot to determine the number of clusters to use. Does the number one would select from the elbow plot match the number of classes in the original dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the number of clusters you determined from previous question to run `slkearn`'s K-Means Clustering with `random_state=2`. Then do the following:\n",
    "\n",
    "- Print the SSE for the K-Means Model.\n",
    "- Plot the predictions in a scatterplot. \n",
    "- Plot the center of each K-means cluster (read the documentation to understand what attribute to use). \n",
    "- Overlay discrepant points between predicted and actual cluster using a different marker shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, we are going to use the toy dataset with \"datasets.make_blobs\" attribution. The goal is to manually implement a K-Means Algorithm and compare with the result obtained in using sklearn.clustering.\n",
    "\n",
    "Fill in the K-Means function below, then run the following cell to see how your function compares to `sklearn`'s. Write your function as follows:\n",
    "\n",
    "1. Initialize an empty dictionary with k keys to store cluster data.\n",
    "2. For each datapoint, calculate its Euclidean distance to each centroid. Assign each point to the cluster with centroid having the smallest distance. \n",
    "3. When all objects have been assigned, recalculate the positions of the k centroids by taking the means of the components of the assigned datapoints. For example, if centroid C1 has points $((1,2),(2,3),(1,3))$ assigned to it, the updated position of C1 would be $\\big(\\frac{1+2+1}{3}$,$\\frac{2+3+3}{3}\\big) = (1.33, 2.67)$\n",
    "4. Repeat until a stopping criterion is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: Data.\n",
    "# k: Number of means to fit.\n",
    "# init_centroids: Initial cluster centers.\n",
    "# tol: Tolerance for difference between old and new cluster-center estimates as measured by Frobenius norm.\n",
    "# max_iter: Maximum number of iterations.\n",
    "def K_Means_Algorithm(X, k, init_centroids, tol, max_iter):\n",
    "    \n",
    "    ### YOUR CODE HERE.\n",
    "\n",
    "    return clusters, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Initialize parameters, load data.\n",
    "k = 4\n",
    "init_centroids  = np.array([[-1.5, -1.0], [0.5, -1.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "n_samples = 100\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, centers=4, cluster_std=1.5, random_state = 99)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "colors = ListedColormap(['red', 'purple', 'green', 'blue'])\n",
    "\n",
    "# Apply your K-Mean Clustering\n",
    "clusters, centroids = K_Means_Algorithm(X, k, init_centroids, tol=1e-4, max_iter=300)\n",
    "y_pred = np.zeros(n_samples)\n",
    "for i in range(n_samples):\n",
    "    for j in range(1,k):\n",
    "        if X[i,:].tolist() in clusters[j]:\n",
    "            y_pred[i] = j\n",
    "\n",
    "# Apply sklearn's K-Means Clustering\n",
    "kmeans = KMeans(verbose=1, n_clusters=k, init=init_centroids, tol=1e-4, max_iter=300, algorithm='full').fit(X)\n",
    "\n",
    "# Visualize \n",
    "plt.figure(figsize = (16,4))\n",
    "ax1  = plt.subplot(1,3,1)\n",
    "ax1.scatter(X[:,0],X[:,1], c = y, cmap=ListedColormap(['black', 'gray', 'pink', 'magenta']))\n",
    "ax1.set_title('Actual Data')\n",
    "\n",
    "ax2  = plt.subplot(1,3,2)\n",
    "ax2.scatter(X[:,0],X[:,1], c=y_pred, cmap=colors)\n",
    "ax2.scatter(centroids[:,0],centroids[:,1],c='k',marker='o',s=300,alpha=0.5,edgecolor='none')\n",
    "ax2.set_title('Manual K-Means')\n",
    "\n",
    "ax3  = plt.subplot(1,3,3)\n",
    "ax3.scatter(X[:,0],X[:,1], c = kmeans.labels_, cmap=colors)\n",
    "C = kmeans.cluster_centers_\n",
    "ax3.scatter(C[:,0],C[:,1],c='k',marker='o',s=300,alpha=0.5,edgecolor='none')\n",
    "ax3.set_title('Sklearn K-Means')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('My centroids:\\n', centroids)\n",
    "print('sklearn\\'s centroids:\\n', C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
