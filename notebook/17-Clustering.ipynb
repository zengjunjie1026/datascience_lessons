{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Introduction to Data Science\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "**_Lecturer_**: **Christian Kuemmerle** - kuemmerle@jhu.edu\n",
    "\n",
    "- Clustering problems\n",
    "- $k$-means clustering\n",
    "- Voronoi tesselation\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Clustering</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning\n",
    "\n",
    "- The methods in general fall in these 4 categories\n",
    "\n",
    ">|                | Supervised     |         Unsupervised     |\n",
    " |----------------|:--------------:|:------------------------:|\n",
    " | **Discrete**   | Classification | Clustering               |   \n",
    " | **Continuous** | Regression     | Dimensionality Reduction |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is a type of machine learning in which the algorithm is not provided with any pre-assigned labels or scores for the training data\n",
    "\n",
    ">$\\displaystyle T = \\big\\{ (x_i) \\big\\}$  where $x_i\\in \\mathbb{R}^d$ are feature sets .\n",
    "\n",
    "#### Recall: Dimensionality Reduction\n",
    "\n",
    "We learnt about **Principal Component Analysis (PCA)**, looking to find a low-dimensional representation of the observations that explain a good fraction of the variance.\n",
    "\n",
    "For example: \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/440px-GaussianScatterPCA.svg.png\" width=300 align=topleft>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering is ...\n",
    "<img src=\"files/clusters.png\" align=right width=200>\n",
    "\n",
    "... the process of collecting a set of objects into groups or clusters of similar items\n",
    "\n",
    "#### For example,\n",
    "\n",
    "- Discover different species of birds based on their photographs\n",
    "- Segment an image based on the pixel colors\n",
    "- Organize news articles that cover the same story\n",
    "\n",
    "<img src=\"http://www.codeproject.com/KB/recipes/439890/clustering-process.png\" width=450 align=left />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2] \n",
    "# only the first 2 features\n",
    "\n",
    "scatter(X[:,0],X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Algorithms\n",
    "\n",
    "- **Flat Clustering**: Set of clusters without explicit structure that would related clusters to each other.\n",
    "\n",
    ">1. Start with a random partitioning\n",
    ">2. Iterate to improve the grouping\n",
    "\n",
    "-> Focus of today's lecture: **k-means**\n",
    "\n",
    "- **Hierarchical Clustering**: Introduces hierarchy between clusters.\n",
    "\n",
    ">1. Greedy grouping of closest data points: bottom up. **Agglomorative Clustering**, e.g. [sklearn.cluster.AgglomerativeClustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    ">2. Greedy splitting of farthest: top down. **Divisive Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat example\n",
    "\n",
    "<img src=\"http://www.codeproject.com/KB/recipes/439890/clustering-process.png\" width=450 align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical example\n",
    "\n",
    "> Set\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Clusters.svg/250px-Clusters.svg.png width=150 align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dendogram\n",
    "\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-means clustering\n",
    "\n",
    "A simple (flat) algorithm. See also [user guide for k-means clustering in scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means as optimization problem\n",
    "\n",
    "Formally, the k-means algorithm (also called Lloyd's algorithm) is nothing but an alternating optimization over the following objective F.\n",
    "\n",
    "For a set of data points $\\{ x_i \\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ for all $i$, we define\n",
    "    \n",
    ">   - $k$ centroids or prototypes $\\mu_1,\\ldots,\\mu_k \\in \\mathbb{R}^d$ (which \"represent\" the \"centers\" of the $k$ clusters), and\n",
    ">   - a partition $C_1 \\cup C_2 \\cup \\ldots \\cup C_k = \\{1,\\ldots,n\\}$ (the $C_j$ are pairwise disjoint), where $C_j$ represents all data point indices which are part of cluster $j$.\n",
    "\n",
    "The objective to minimize is then\n",
    "\n",
    "$$ F\\big(\\{ \\mu_j \\}_{j=1}^k, (C_j)_{j=1}^k \\big) = \\sum_{j=1}^k \\left[\\ \\sum_{x_i\\in{}C_j}\\ \\lvert\\!\\lvert x_i-\\mu_j\\rvert\\!\\rvert^2 \\right]. $$\n",
    "\n",
    "For a given set of centroids $\\mu_1,\\ldots,\\mu_k$, we update then\n",
    ">$\\displaystyle  \\hat{{C}} = (\\hat{{C}}_j)_{j=1}^k =  \\underset{{{C} \\text{ is partition}}}{\\operatorname{argmin}} F\\big(\\{ \\mu_j \\}_{j=1}^k, (C_j)_{j=1}^k \\big) = \\underset{{{C} \\text{ is partition}}}{\\operatorname{argmin}} \\sum_{j=1}^k \\left[\\ \\sum_{x_i\\in{}C_j}\\ \\lvert\\!\\lvert x_i-\\mu_j\\rvert\\!\\rvert^2 \\right] $\n",
    "\n",
    "and, subsequently, for a given, updated partition $(C_j)_{j=1}^k$, we update the centroids $\\{ \\mu_j \\}_{j=1}^k$ such that \n",
    "> $\\displaystyle (\\hat{\\mu}_1,\\ldots,\\hat{\\mu}_k) = \\underset{\\{ \\mu_j \\}_{j=1}^k}{\\operatorname{argmin}}    F\\big(\\{ \\mu_j \\}_{j=1}^k, (C_j)_{j=1}^k \\big) =  \\underset{\\{ \\mu_j \\}_{j=1}^k}{\\operatorname{argmin}}  \\sum_{j=1}^k \\left[\\ \\sum_{x_i\\in{}C_j}\\ \\lvert\\!\\lvert x_i-\\mu_j\\rvert\\!\\rvert^2 \\right]$\n",
    "\n",
    "which leads for each $j=1,\\ldots,k$ to the intra-cluster average\n",
    ">$$\\displaystyle  \\hat{\\mu}_j = \\frac{1}{\\lvert{C_j}\\rvert}\\sum_{x_i\\in{}C_j} x_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "- Iteratively improving the $\\mu_i$ **prototypes** of $k$ clusters\n",
    "\n",
    ">1. Pick $k$ random objects as the initial $\\mu_j$ prototypes\n",
    ">0. Find for each data point $x_i$ the closest prototype $\\mu_j$ and assign to that cluster $C_j$\n",
    ">0. Calculate the averages for each cluster $C_j$ to get new $\\mu_j$\n",
    ">0. Repeat until convergence\n",
    "\n",
    "Step 2. and 3. are the steps above described in formulas.\n",
    "\n",
    "- Often very fast - but no proof\n",
    "\n",
    "In scikit-learn:\n",
    "\n",
    "[sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation\n",
    "\n",
    "<img src=files/kmeans.gif>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Click on these <a href=http://shabal.in/visuals/kmeans/1.html>animations</a> to see the process of $k$-means clustering in action\n",
    " \n",
    "> Starting from different points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(6,6)); ax=subplot(aspect='equal')\n",
    "scatter(X[:,0],X[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(init='random', n_clusters=3, n_init=100)\n",
    "kmeans.fit(X)\n",
    "\n",
    "figure(figsize=(6,6)); ax=subplot(aspect='equal')\n",
    "scatter(X[:,0],X[:,1],c=kmeans.labels_,cmap=cm.rainbow);\n",
    "\n",
    "C = kmeans.cluster_centers_\n",
    "scatter(C[:,0],C[:,1],c='k',marker='o',s=300,alpha=0.5,edgecolor='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_[xx.ravel(), yy.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step size of the mesh. \n",
    "h = .005    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = X[:, 0].min() -.5, X[:, 0].max() +.5\n",
    "y_min, y_max = X[:, 1].min() -.5, X[:, 1].max() +.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "P = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "P = P.reshape(xx.shape)\n",
    "figure(figsize=(6,6)); subplot(111,aspect='equal')\n",
    "plt.clf()\n",
    "\n",
    "plt.imshow(P, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='equal', origin='lower', alpha=0.7)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c='k', alpha=0.7)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10, alpha=0.8)\n",
    "\n",
    "plt.xlim(x_min, x_max);\n",
    "plt.ylim(y_min, y_max);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Voronoi Tessellation & Delaunay Triangulation\n",
    "\n",
    "- _Voronoi tessellation_: Color areas according to which $\\mu_j$ is closest\n",
    "\n",
    "- _Delaunay triangulation_: Define a triangulation from triangles defined by the $\\mu_j$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/1200px-Euclidean_Voronoi_diagram.svg.png\" alt=\"Euclidean Voronoi diagram.svg\" align=left width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "- Initialization matters\n",
    "\n",
    "> Rerun multiple times: **n_init** (default=10) <br/>\n",
    "> Smart(er) starting points\n",
    "\n",
    "<img src=\"https://files.realpython.com/media/centroids_iterations.247379590275.gif\" alt=\"k-means algorithm\" align=left width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means++ (default initialization in [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html))\n",
    "\n",
    "- Choose one center $\\mu_1$ uniformly at random among the data points.\n",
    "- For each data point $x$ not chosen yet, compute $D(x)$, the distance between $x$ and the nearest center $\\mu_i$   that has already been chosen.\n",
    "- Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to $D(x)^2$.\n",
    "- Repeat Steps 2 and 3 until $k$ centers have been chosen.\n",
    "- Now that the initial centers have been chosen, proceed using standard k-means clustering.\n",
    "\n",
    "Alternative: Choose input parameter `init='random'` for random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters=2, n_init=100)\n",
    "kmeans.fit(X)\n",
    "\n",
    "figure(figsize=(6,6)); ax=subplot(aspect='equal')\n",
    "scatter(X[:,0],X[:,1],c=kmeans.labels_,cmap=cm.rainbow);\n",
    "\n",
    "C = kmeans.cluster_centers_\n",
    "scatter(C[:,0],C[:,1],c='k',marker='o',s=300,alpha=0.5,edgecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential issue:\n",
    "\n",
    "- K-means assumes spherical clusters (use of distance function)\n",
    "\n",
    "> Preprocessing becomes important even in simple cases <br>\n",
    "> For example, whitening..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations might help\n",
    "Z = X.copy()\n",
    "Z[:,0] *= 0.5\n",
    "\n",
    "kmeans = KMeans(n_clusters=2,n_init=100,init='k-means++')\n",
    "kmeans.fit(Z)\n",
    "C, L = kmeans.cluster_centers_, kmeans.labels_\n",
    "\n",
    "figure(figsize=(6,6)); ax=subplot(aspect='equal')\n",
    "scatter(Z[:,0],Z[:,1],c= L, marker='o',s= 30,alpha=0.7,cmap=cm.rainbow);\n",
    "scatter(C[:,0],C[:,1],c='k',marker='o',s=300,alpha=0.5,edgecolor='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this simple transformation, the cluster assignment has changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means is limited to linear cluster boundaries\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(200, noise=.05, random_state=0)\n",
    "labels = KMeans(2, random_state=0).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem mitigated by other clustering methods: Kernel transformation, spectral clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What $k$?\n",
    "\n",
    "- How many clusters?\n",
    "\n",
    "> Too many? <br/>\n",
    "> Too few?\n",
    "\n",
    "- Various diagnostics\n",
    "\n",
    "> Check the minimum value of the cost function? <br/>\n",
    "> Characterize the clusters - Gaussian? spherical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate 100 not perfectly separable points\n",
    "X, y = make_blobs(n_samples=200, centers=3, cluster_std=2.75, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "plt.scatter(X_scaled[:,0], X_scaled[:,1], c=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = [] # A list holds the SSE values for each k\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=100)\n",
    "    kmeans.fit(X)\n",
    "    sse.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Elbow Method\n",
    ">\n",
    ">Select the value of $k$ at the elbow, i.e., the point after which the minimum of distortion value starts to decrease slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "kl = KneeLocator( range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "kl.elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Silhouette Method: [silhouette coefficient](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) values range between -1 and 1 and quantifies how well a data point fits into its assigned cluster based on two factors:\n",
    "\n",
    "> How close the data point is to other points in the cluster\n",
    ">\n",
    "> How far away the data point is from points in other clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_scores = [] \n",
    "    \n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=100)\n",
    "    kmeans.fit(X)\n",
    "    score = silhouette_score(X, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Lineplot using silhouette score\n",
    "plt.plot(range(2, 11), silhouette_scores) \n",
    "plt.xlabel('K - Number of Clusters') \n",
    "plt.ylabel('Silhouette score') \n",
    "plt.title('The Silhouette Method') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Run $k$-means on this [CSV](files/Class-Clusters.csv) file\n",
    "- Try different parameters\n",
    "- How many clusters did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = loadtxt('files/Class-Clusters.csv', delimiter=',')\n",
    "scatter(X[:,0],X[:,1],s=50,alpha=0.3,edgecolor='none');\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-medians clustering\n",
    "\n",
    "Replace mean with median for cluster centers: Instead of (squared) Euclidean distance ($L_2$), use Taxicab ($L_1$) distance."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
